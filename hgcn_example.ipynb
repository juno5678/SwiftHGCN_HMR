{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e4a28f5-5b60-4251-95ad-378cb3da0ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juno/anaconda3/envs/hgcn/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from torch_geometric.experimental import disable_dynamic_shapes\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.utils import scatter, softmax\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3cc1b5d-872f-499e-bf22-3badc058bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_HGConv_node_to_edge(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        use_attention: bool = False,\n",
    "        attention_mode: str = 'node',\n",
    "        heads: int = 1,\n",
    "        concat: bool = True,\n",
    "        negative_slope: float = 0.2,\n",
    "        dropout: float = 0,\n",
    "        bias: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super().__init__(flow='source_to_target', node_dim=0, **kwargs)\n",
    "\n",
    "        assert attention_mode in ['node', 'edge']\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_attention = use_attention\n",
    "        self.attention_mode = attention_mode\n",
    "        self.heads = 1\n",
    "        self.concat = 1\n",
    "        \n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.empty(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.empty(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        if self.use_attention:\n",
    "            glorot(self.att)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    @disable_dynamic_shapes(required_args=['num_edges'])\n",
    "    def forward(self,x: Tensor, hyperedge_index: Tensor,\n",
    "                hyperedge_weight: Optional[Tensor] = None,\n",
    "                hyperedge_attr: Optional[Tensor] = None,\n",
    "                num_edges: Optional[int] = None) -> Tensor:\n",
    "        r\"\"\"Runs the forward pass of the module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Node feature matrix\n",
    "                :math:`\\mathbf{X} \\in \\mathbb{R}^{N \\times F}`.\n",
    "            hyperedge_index (torch.Tensor): The hyperedge indices, *i.e.*\n",
    "                the sparse incidence matrix\n",
    "                :math:`\\mathbf{H} \\in {\\{ 0, 1 \\}}^{N \\times M}` mapping from\n",
    "                nodes to edges.\n",
    "            hyperedge_weight (torch.Tensor, optional): Hyperedge weights\n",
    "                :math:`\\mathbf{W} \\in \\mathbb{R}^M`. (default: :obj:`None`)\n",
    "            hyperedge_attr (torch.Tensor, optional): Hyperedge feature matrix\n",
    "                in :math:`\\mathbb{R}^{M \\times F}`.\n",
    "                These features only need to get passed in case\n",
    "                :obj:`use_attention=True`. (default: :obj:`None`)\n",
    "            num_edges (int, optional) : The number of edges :math:`M`.\n",
    "                (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        num_nodes = x.size(0)\n",
    "\n",
    "        if num_edges is None:\n",
    "            num_edges = 0\n",
    "            if hyperedge_index.numel() > 0:\n",
    "                num_edges = int(hyperedge_index[1].max()) + 1\n",
    "\n",
    "        if hyperedge_weight is None:\n",
    "            hyperedge_weight = x.new_ones(num_edges)\n",
    "\n",
    "        alpha = None\n",
    "\n",
    "        B = scatter(x.new_ones(hyperedge_index.size(1)), hyperedge_index[1],\n",
    "                    dim=0, dim_size=num_edges, reduce='sum')\n",
    "        B = 1.0 / B\n",
    "        B[B == float(\"inf\")] = 0\n",
    "\n",
    "        out = self.propagate(hyperedge_index, x=x, norm=B, alpha=alpha,\n",
    "                             size=(num_nodes, num_edges))\n",
    "        print(\"out : \", out)\n",
    "\n",
    "        if self.concat is True:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        return out, num_edges, num_nodes, hyperedge_weight\n",
    "    def message(self, x_j: Tensor, norm_i: Tensor, alpha: Tensor) -> Tensor:\n",
    "        H, F = self.heads, self.out_channels\n",
    "\n",
    "        out = norm_i.view(-1, 1, 1) * x_j.view(-1, H, F)\n",
    "\n",
    "        if alpha is not None:\n",
    "            out = alpha.view(-1, self.heads, 1) * out\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d91d4c72-1c0c-4186-9864-1840e834f3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_HGConv_edge_to_node(MessagePassing):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            use_attention: bool = False,\n",
    "            attention_mode: str = 'node',\n",
    "            heads: int = 1,\n",
    "            concat: bool = True,\n",
    "            negative_slope: float = 0.2,\n",
    "            dropout: float = 0,\n",
    "            bias: bool = True,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super().__init__(flow='source_to_target', node_dim=0, **kwargs)\n",
    "\n",
    "        assert attention_mode in ['node', 'edge']\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_attention = use_attention\n",
    "        self.attention_mode = attention_mode\n",
    "        self.heads = 1\n",
    "        self.concat = 1\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.empty(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.empty(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        if self.use_attention:\n",
    "            glorot(self.att)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    @disable_dynamic_shapes(required_args=['num_edges'])\n",
    "    def forward(self, x: Tensor, hyperedge_index: Tensor,\n",
    "                hyperedge_weight: Optional[Tensor] = None,\n",
    "                hyperedge_attr: Optional[Tensor] = None,\n",
    "                num_edges: Optional[int] = None, num_nodes: Optional[int] = None) -> Tensor:\n",
    "        r\"\"\"Runs the forward pass of the module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Node feature matrix\n",
    "                :math:`\\mathbf{X} \\in \\mathbb{R}^{N \\times F}`.\n",
    "            hyperedge_index (torch.Tensor): The hyperedge indices, *i.e.*\n",
    "                the sparse incidence matrix\n",
    "                :math:`\\mathbf{H} \\in {\\{ 0, 1 \\}}^{N \\times M}` mapping from\n",
    "                nodes to edges.\n",
    "            hyperedge_weight (torch.Tensor, optional): Hyperedge weights\n",
    "                :math:`\\mathbf{W} \\in \\mathbb{R}^M`. (default: :obj:`None`)\n",
    "            hyperedge_attr (torch.Tensor, optional): Hyperedge feature matrix\n",
    "                in :math:`\\mathbb{R}^{M \\times F}`.\n",
    "                These features only need to get passed in case\n",
    "                :obj:`use_attention=True`. (default: :obj:`None`)\n",
    "            num_edges (int, optional) : The number of edges :math:`M`.\n",
    "                (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "\n",
    "        # num_nodes = x.size(0)\n",
    "\n",
    "        if num_edges is None:\n",
    "            num_edges = 0\n",
    "            if hyperedge_index.numel() > 0:\n",
    "                num_edges = int(hyperedge_index[1].max()) + 1\n",
    "\n",
    "        if hyperedge_weight is None:\n",
    "            hyperedge_weight = x.new_ones(num_edges)\n",
    "\n",
    "\n",
    "        alpha = None\n",
    "        \n",
    "        D = scatter(hyperedge_weight[hyperedge_index[1]], hyperedge_index[0],\n",
    "                    dim=0, dim_size=num_nodes, reduce='sum')\n",
    "        D = 1.0 / D\n",
    "        D[D == float(\"inf\")] = 0\n",
    "\n",
    "\n",
    "        out = self.propagate(hyperedge_index.flip([0]), x=x, norm=D,\n",
    "                             alpha=alpha, size=(num_edges, num_nodes))\n",
    "\n",
    "        if self.concat is True:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j: Tensor, norm_i: Tensor, alpha: Tensor) -> Tensor:\n",
    "        H, F = self.heads, self.out_channels\n",
    "\n",
    "        out = norm_i.view(-1, 1, 1) * x_j.view(-1, H, F)\n",
    "\n",
    "        if alpha is not None:\n",
    "            out = alpha.view(-1, self.heads, 1) * out\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b457e005-e196-4c65-88c6-72118c515710",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleHGCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleHGCN, self).__init__()\n",
    "        self.n2e = test_HGConv_node_to_edge(in_channels=1, out_channels=1, use_attention=False)\n",
    "        self.e2n = test_HGConv_edge_to_node(in_channels=1, out_channels=1, use_attention=False)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # 节点到超边的信息传递\n",
    "        hyperedge_feats = self.n2e(x, edge_index)\n",
    "        # 超边到节点的信息传递\n",
    "        node_feats = self.e2n(hyperedge_feats[0], edge_index)\n",
    "        # return hyperedge_feats\n",
    "        return hyperedge_feats, node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "111f6f2f-0ce5-4b75-8037-5a7423250f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :  tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.]])\n",
      "out :  tensor([[[3.]],\n",
      "\n",
      "        [[3.]],\n",
      "\n",
      "        [[5.]]])\n",
      "Node to Hyperedge Information Transfer Result:\n",
      "(tensor([[3.],\n",
      "        [3.],\n",
      "        [5.]], grad_fn=<AddBackward0>), 3, 6, tensor([1., 1., 1.]))\n",
      "\n",
      "Hyperedge to Node Information Transfer Result:\n",
      "tensor([[3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [4.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HypergraphConv\n",
    "\n",
    "# 构建 6 个节点和 3 个超边的 incident matrix\n",
    "num_nodes = 6\n",
    "num_hyperedges = 3\n",
    "\n",
    "# incident matrix shape: (num_nodes, num_hyperedges)\n",
    "# 假设以下 incident matrix (手动设置一个示例)\n",
    "incidence_matrix = torch.tensor([\n",
    "    [1, 0, 0],  # node 1 connected to hyperedge 1\n",
    "    [1, 1, 0],  # node 2 connected to hyperedge 1 and 2\n",
    "    [0, 1, 0],  # node 3 connected to hyperedge 2\n",
    "    [0, 1, 1],  # node 4 connected to hyperedge 2 and 3\n",
    "    [0, 0, 1],  # node 5 connected to hyperedge 3\n",
    "    [1, 0, 1]   # node 6 connected to hyperedge 1 and 3\n",
    "], dtype=torch.float)\n",
    "\n",
    "# 转换为边索引\n",
    "edge_index = incidence_matrix.nonzero().t()\n",
    "\n",
    "# 节点特征 (假设 6 个节点，每个节点特征维度为 1，且初始值为 1)\n",
    "# x = torch.ones((num_nodes, 1))\n",
    "x = torch.arange(1, num_nodes + 1, dtype=torch.float).view(-1, 1)\n",
    "print(\"x : \", x)\n",
    "# 初始化模型\n",
    "model = SimpleHGCN()\n",
    "\n",
    "# 前向传播\n",
    "hyperedge_feats, node_feats = model(x, edge_index)\n",
    "# hyperedge_feats = model(x, edge_index)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Node to Hyperedge Information Transfer Result:\")\n",
    "print(hyperedge_feats)\n",
    "\n",
    "\n",
    "print(\"\\nHyperedge to Node Information Transfer Result:\")\n",
    "print(node_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd990226-9932-4168-b82e-b308fccc95d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from torch_geometric.experimental import disable_dynamic_shapes\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.utils import scatter, softmax\n",
    "\n",
    "\n",
    "class test_HypergraphConv(MessagePassing):\n",
    "    r\"\"\"The hypergraph convolutional operator from the `\"Hypergraph Convolution\n",
    "    and Hypergraph Attention\" <https://arxiv.org/abs/1901.08150>`_ paper.\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{X}^{\\prime} = \\mathbf{D}^{-1} \\mathbf{H} \\mathbf{W}\n",
    "        \\mathbf{B}^{-1} \\mathbf{H}^{\\top} \\mathbf{X} \\mathbf{\\Theta}\n",
    "\n",
    "    where :math:`\\mathbf{H} \\in {\\{ 0, 1 \\}}^{N \\times M}` is the incidence\n",
    "    matrix, :math:`\\mathbf{W} \\in \\mathbb{R}^M` is the diagonal hyperedge\n",
    "    weight matrix, and\n",
    "    :math:`\\mathbf{D}` and :math:`\\mathbf{B}` are the corresponding degree\n",
    "    matrices.\n",
    "\n",
    "    For example, in the hypergraph scenario\n",
    "    :math:`\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})` with\n",
    "    :math:`\\mathcal{V} = \\{ 0, 1, 2, 3 \\}` and\n",
    "    :math:`\\mathcal{E} = \\{ \\{ 0, 1, 2 \\}, \\{ 1, 2, 3 \\} \\}`, the\n",
    "    :obj:`hyperedge_index` is represented as:\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        hyperedge_index = torch.tensor([\n",
    "            [0, 1, 2, 1, 2, 3],\n",
    "            [0, 0, 0, 1, 1, 1],\n",
    "        ])\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample, or :obj:`-1` to derive\n",
    "            the size from the first input(s) to the forward method.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        use_attention (bool, optional): If set to :obj:`True`, attention\n",
    "            will be added to this layer. (default: :obj:`False`)\n",
    "        attention_mode (str, optional): The mode on how to compute attention.\n",
    "            If set to :obj:`\"node\"`, will compute attention scores of nodes\n",
    "            within all nodes belonging to the same hyperedge.\n",
    "            If set to :obj:`\"edge\"`, will compute attention scores of nodes\n",
    "            across all edges holding this node belongs to.\n",
    "            (default: :obj:`\"node\"`)\n",
    "        heads (int, optional): Number of multi-head-attentions.\n",
    "            (default: :obj:`1`)\n",
    "        concat (bool, optional): If set to :obj:`False`, the multi-head\n",
    "            attentions are averaged instead of concatenated.\n",
    "            (default: :obj:`True`)\n",
    "        negative_slope (float, optional): LeakyReLU angle of the negative\n",
    "            slope. (default: :obj:`0.2`)\n",
    "        dropout (float, optional): Dropout probability of the normalized\n",
    "            attention coefficients which exposes each node to a stochastically\n",
    "            sampled neighborhood during training. (default: :obj:`0`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "\n",
    "    Shapes:\n",
    "        - **input:**\n",
    "          node features :math:`(|\\mathcal{V}|, F_{in})`,\n",
    "          hyperedge indices :math:`(|\\mathcal{V}|, |\\mathcal{E}|)`,\n",
    "          hyperedge weights :math:`(|\\mathcal{E}|)` *(optional)*\n",
    "          hyperedge features :math:`(|\\mathcal{E}|, D)` *(optional)*\n",
    "        - **output:** node features :math:`(|\\mathcal{V}|, F_{out})`\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        use_attention: bool = False,\n",
    "        attention_mode: str = 'node',\n",
    "        heads: int = 1,\n",
    "        concat: bool = True,\n",
    "        negative_slope: float = 0.2,\n",
    "        dropout: float = 0,\n",
    "        bias: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super().__init__(flow='source_to_target', node_dim=0, **kwargs)\n",
    "\n",
    "        assert attention_mode in ['node', 'edge']\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_attention = use_attention\n",
    "        self.attention_mode = attention_mode\n",
    "\n",
    "        if self.use_attention:\n",
    "            self.heads = heads\n",
    "            self.concat = concat\n",
    "            self.negative_slope = negative_slope\n",
    "            self.dropout = dropout\n",
    "            self.lin = Linear(in_channels, heads * out_channels, bias=False,\n",
    "                              weight_initializer='glorot')\n",
    "            self.att = Parameter(torch.empty(1, heads, 2 * out_channels))\n",
    "        else:\n",
    "            self.heads = 1\n",
    "            self.concat = True\n",
    "            self.lin = Linear(in_channels, out_channels, bias=False,\n",
    "                              weight_initializer='glorot')\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.empty(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.empty(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        self.lin.reset_parameters()\n",
    "        if self.use_attention:\n",
    "            glorot(self.att)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    @disable_dynamic_shapes(required_args=['num_edges'])\n",
    "    def forward(self, x: Tensor, hyperedge_index: Tensor,\n",
    "                hyperedge_weight: Optional[Tensor] = None,\n",
    "                hyperedge_attr: Optional[Tensor] = None,\n",
    "                num_edges: Optional[int] = None) -> Tensor:\n",
    "        r\"\"\"Runs the forward pass of the module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Node feature matrix\n",
    "                :math:`\\mathbf{X} \\in \\mathbb{R}^{N \\times F}`.\n",
    "            hyperedge_index (torch.Tensor): The hyperedge indices, *i.e.*\n",
    "                the sparse incidence matrix\n",
    "                :math:`\\mathbf{H} \\in {\\{ 0, 1 \\}}^{N \\times M}` mapping from\n",
    "                nodes to edges.\n",
    "            hyperedge_weight (torch.Tensor, optional): Hyperedge weights\n",
    "                :math:`\\mathbf{W} \\in \\mathbb{R}^M`. (default: :obj:`None`)\n",
    "            hyperedge_attr (torch.Tensor, optional): Hyperedge feature matrix\n",
    "                in :math:`\\mathbb{R}^{M \\times F}`.\n",
    "                These features only need to get passed in case\n",
    "                :obj:`use_attention=True`. (default: :obj:`None`)\n",
    "            num_edges (int, optional) : The number of edges :math:`M`.\n",
    "                (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        num_nodes = x.size(0)\n",
    "\n",
    "        if num_edges is None:\n",
    "            num_edges = 0\n",
    "            if hyperedge_index.numel() > 0:\n",
    "                num_edges = int(hyperedge_index[1].max()) + 1\n",
    "\n",
    "        if hyperedge_weight is None:\n",
    "            hyperedge_weight = x.new_ones(num_edges)\n",
    "\n",
    "        # x = self.lin(x)\n",
    "\n",
    "        alpha = None\n",
    "        if self.use_attention:\n",
    "            assert hyperedge_attr is not None\n",
    "            x = x.view(-1, self.heads, self.out_channels)\n",
    "            hyperedge_attr = self.lin(hyperedge_attr)\n",
    "            hyperedge_attr = hyperedge_attr.view(-1, self.heads,\n",
    "                                                 self.out_channels)\n",
    "            x_i = x[hyperedge_index[0]]\n",
    "            x_j = hyperedge_attr[hyperedge_index[1]]\n",
    "            alpha = (torch.cat([x_i, x_j], dim=-1) * self.att).sum(dim=-1)\n",
    "            alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "            if self.attention_mode == 'node':\n",
    "                alpha = softmax(alpha, hyperedge_index[1], num_nodes=num_edges)\n",
    "            else:\n",
    "                alpha = softmax(alpha, hyperedge_index[0], num_nodes=num_nodes)\n",
    "            alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "\n",
    "        D = scatter(hyperedge_weight[hyperedge_index[1]], hyperedge_index[0],\n",
    "                    dim=0, dim_size=num_nodes, reduce='sum')\n",
    "        print(\"before node degree D : \", D)\n",
    "        D = 1.0 / D\n",
    "        D[D == float(\"inf\")] = 0\n",
    "        print(\"after node degree D : \", D)\n",
    "\n",
    "        B = scatter(x.new_ones(hyperedge_index.size(1)), hyperedge_index[1],\n",
    "                    dim=0, dim_size=num_edges, reduce='sum')\n",
    "        print(\"before edge degree B : \", B)\n",
    "        B = 1.0 / B\n",
    "        B[B == float(\"inf\")] = 0\n",
    "        print(\"after edge degree B : \", B)\n",
    "\n",
    "        out = self.propagate(hyperedge_index, x=x, norm=B, alpha=alpha,\n",
    "                             size=(num_nodes, num_edges))\n",
    "        print(\"node to edge : \", out)\n",
    "        out = self.propagate(hyperedge_index.flip([0]), x=out, norm=D,\n",
    "                             alpha=alpha, size=(num_edges, num_nodes))\n",
    "        print(\"edge to node : \", out)\n",
    "        \n",
    "        if self.concat is True:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j: Tensor, norm_i: Tensor, alpha: Tensor) -> Tensor:\n",
    "        H, F = self.heads, self.out_channels\n",
    "\n",
    "        out = norm_i.view(-1, 1, 1) * x_j.view(-1, H, F)\n",
    "\n",
    "        if alpha is not None:\n",
    "            out = alpha.view(-1, self.heads, 1) * out\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fa403d60-457f-4334-8b31-6da23c2ae770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_SimpleHGCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(test_SimpleHGCN, self).__init__()\n",
    "        self.hgcn = test_HypergraphConv(in_channels=1, out_channels=1, use_attention=False)\n",
    "        # self.e2n = test_HGConv_edge_to_node(in_channels=1, out_channels=1, use_attention=False)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # 节点到超边的信息传递\n",
    "        hyperedge_feats = self.hgcn(x, edge_index)\n",
    "        # 超边到节点的信息传递\n",
    "        # node_feats = self.e2n(hyperedge_feats[0], edge_index)\n",
    "        return hyperedge_feats\n",
    "        # return hyperedge_feats, node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2be51786-c41c-4ebe-b6da-ccec447366ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :  tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.]])\n",
      "before node degree D :  tensor([1., 1., 2., 2., 1., 1.])\n",
      "after node degree D :  tensor([1.0000, 1.0000, 0.5000, 0.5000, 1.0000, 1.0000])\n",
      "before edge degree B :  tensor([2., 3., 3.])\n",
      "after edge degree B :  tensor([0.5000, 0.3333, 0.3333])\n",
      "node to edge :  tensor([[[2.]],\n",
      "\n",
      "        [[3.]],\n",
      "\n",
      "        [[5.]]])\n",
      "edge to node :  tensor([[[2.0000]],\n",
      "\n",
      "        [[3.0000]],\n",
      "\n",
      "        [[2.5000]],\n",
      "\n",
      "        [[4.0000]],\n",
      "\n",
      "        [[5.0000]],\n",
      "\n",
      "        [[5.0000]]])\n",
      "\n",
      "Hyperedge to Node Information Transfer Result:\n",
      "tensor([[2.0000],\n",
      "        [3.0000],\n",
      "        [2.5000],\n",
      "        [4.0000],\n",
      "        [5.0000],\n",
      "        [5.0000]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HypergraphConv\n",
    "\n",
    "# 构建 6 个节点和 3 个超边的 incident matrix\n",
    "num_nodes = 6\n",
    "num_hyperedges = 3\n",
    "\n",
    "# incident matrix shape: (num_nodes, num_hyperedges)\n",
    "# 假设以下 incident matrix (手动设置一个示例)\n",
    "incidence_matrix = torch.tensor([\n",
    "    [1, 0, 0],  # node 1 connected to hyperedge 1\n",
    "    [0, 1, 0],  # node 2 connected to hyperedge 1 and 2\n",
    "    [1, 1, 0],  # node 3 connected to hyperedge 2\n",
    "    [0, 1, 1],  # node 4 connected to hyperedge 2 and 3\n",
    "    [0, 0, 1],  # node 5 connected to hyperedge 3\n",
    "    [0, 0, 1]   # node 6 connected to hyperedge 1 and 3\n",
    "], dtype=torch.float)\n",
    "\n",
    "# 转换为边索引\n",
    "edge_index = incidence_matrix.nonzero().t()\n",
    "\n",
    "# 节点特征 (假设 6 个节点，每个节点特征维度为 1，且初始值为 1)\n",
    "# x = torch.ones((num_nodes, 1))\n",
    "x = torch.arange(1, num_nodes + 1, dtype=torch.float).view(-1, 1)\n",
    "print(\"x : \", x)\n",
    "# 初始化模型\n",
    "model = test_SimpleHGCN()\n",
    "\n",
    "# 前向传播\n",
    "node_feats = model(x, edge_index)\n",
    "# hyperedge_feats = model(x, edge_index)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Node to Hyperedge Information Transfer Result:\")\n",
    "# print(hyperedge_feats)\n",
    "\n",
    "\n",
    "print(\"\\nHyperedge to Node Information Transfer Result:\")\n",
    "print(node_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "72b8a248-7798-4dfa-a17a-67370d54b147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before node degree D :  tensor([1., 1., 2., 2., 1., 1.])\n",
      "after node degree D :  tensor([1.0000, 1.0000, 0.5000, 0.5000, 1.0000, 1.0000])\n",
      "before edge degree B :  tensor([2., 3., 3.])\n",
      "after edge degree B :  tensor([0.5000, 0.3333, 0.3333])\n",
      "node to edge :  tensor([[[2.2500]],\n",
      "\n",
      "        [[3.1667]],\n",
      "\n",
      "        [[4.6667]]], grad_fn=<ScatterAddBackward0>)\n",
      "edge to node :  tensor([[[2.2500]],\n",
      "\n",
      "        [[3.1667]],\n",
      "\n",
      "        [[2.7083]],\n",
      "\n",
      "        [[3.9167]],\n",
      "\n",
      "        [[4.6667]],\n",
      "\n",
      "        [[4.6667]]], grad_fn=<ScatterAddBackward0>)\n",
      "\n",
      "Hyperedge to Node Information Transfer Result:\n",
      "tensor([[2.2500],\n",
      "        [3.1667],\n",
      "        [2.7083],\n",
      "        [3.9167],\n",
      "        [4.6667],\n",
      "        [4.6667]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 前向传播\n",
    "node_feats = model(node_feats, edge_index)\n",
    "# hyperedge_feats = model(x, edge_index)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Node to Hyperedge Information Transfer Result:\")\n",
    "# print(hyperedge_feats)\n",
    "\n",
    "\n",
    "print(\"\\nHyperedge to Node Information Transfer Result:\")\n",
    "print(node_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f6373a97-b897-46f7-8a44-4bedb6cd4250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before node degree D :  tensor([1., 1., 2., 2., 1., 1.])\n",
      "after node degree D :  tensor([1.0000, 1.0000, 0.5000, 0.5000, 1.0000, 1.0000])\n",
      "before edge degree B :  tensor([2., 3., 3.])\n",
      "after edge degree B :  tensor([0.5000, 0.3333, 0.3333])\n",
      "node to edge :  tensor([[[2.4792]],\n",
      "\n",
      "        [[3.2639]],\n",
      "\n",
      "        [[4.4167]]], grad_fn=<ScatterAddBackward0>)\n",
      "edge to node :  tensor([[[2.4792]],\n",
      "\n",
      "        [[3.2639]],\n",
      "\n",
      "        [[2.8715]],\n",
      "\n",
      "        [[3.8403]],\n",
      "\n",
      "        [[4.4167]],\n",
      "\n",
      "        [[4.4167]]], grad_fn=<ScatterAddBackward0>)\n",
      "\n",
      "Hyperedge to Node Information Transfer Result:\n",
      "tensor([[2.4792],\n",
      "        [3.2639],\n",
      "        [2.8715],\n",
      "        [3.8403],\n",
      "        [4.4167],\n",
      "        [4.4167]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 前向传播\n",
    "node_feats = model(node_feats, edge_index)\n",
    "# hyperedge_feats = model(x, edge_index)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Node to Hyperedge Information Transfer Result:\")\n",
    "# print(hyperedge_feats)\n",
    "\n",
    "\n",
    "print(\"\\nHyperedge to Node Information Transfer Result:\")\n",
    "print(node_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "263b348a-9ac9-4496-8761-485418ba1610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn.inits import zeros\n",
    "from torch_geometric.typing import (\n",
    "    Adj,\n",
    "    OptPairTensor,\n",
    "    OptTensor,\n",
    "    SparseTensor,\n",
    "    torch_sparse,\n",
    ")\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "from torch_geometric.utils import add_self_loops as add_self_loops_fn\n",
    "from torch_geometric.utils import (\n",
    "    is_torch_sparse_tensor,\n",
    "    scatter,\n",
    "    spmm,\n",
    "    to_edge_index,\n",
    ")\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_geometric.utils.sparse import set_sparse_value\n",
    "\n",
    "torch.jit._overload\n",
    "def gcn_norm(  # noqa: F811\n",
    "        edge_index, edge_weight, num_nodes, improved, add_self_loops, flow,\n",
    "        dtype):\n",
    "    # type: (SparseTensor, OptTensor, Optional[int], bool, bool, str, Optional[int]) -> SparseTensor  # noqa\n",
    "    pass\n",
    "\n",
    "\n",
    "def gcn_norm(  # noqa: F811\n",
    "    edge_index: Adj,\n",
    "    edge_weight: OptTensor = None,\n",
    "    num_nodes: Optional[int] = None,\n",
    "    improved: bool = False,\n",
    "    add_self_loops: bool = True,\n",
    "    flow: str = \"source_to_target\",\n",
    "    dtype: Optional[torch.dtype] = None,\n",
    "):\n",
    "    fill_value = 2. if improved else 1.\n",
    "\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        assert edge_index.size(0) == edge_index.size(1)\n",
    "\n",
    "        adj_t = edge_index\n",
    "\n",
    "        if not adj_t.has_value():\n",
    "            adj_t = adj_t.fill_value(1., dtype=dtype)\n",
    "        if add_self_loops:\n",
    "            adj_t = torch_sparse.fill_diag(adj_t, fill_value)\n",
    "\n",
    "        deg = torch_sparse.sum(adj_t, dim=1)\n",
    "        deg_inv_sqrt = deg.pow_(-0.5)\n",
    "        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0.)\n",
    "        adj_t = torch_sparse.mul(adj_t, deg_inv_sqrt.view(-1, 1))\n",
    "        adj_t = torch_sparse.mul(adj_t, deg_inv_sqrt.view(1, -1))\n",
    "\n",
    "        return adj_t\n",
    "\n",
    "    if is_torch_sparse_tensor(edge_index):\n",
    "        assert edge_index.size(0) == edge_index.size(1)\n",
    "\n",
    "        if edge_index.layout == torch.sparse_csc:\n",
    "            raise NotImplementedError(\"Sparse CSC matrices are not yet \"\n",
    "                                      \"supported in 'gcn_norm'\")\n",
    "\n",
    "        adj_t = edge_index\n",
    "        if add_self_loops:\n",
    "            adj_t, _ = add_self_loops_fn(adj_t, None, fill_value, num_nodes)\n",
    "\n",
    "        edge_index, value = to_edge_index(adj_t)\n",
    "        col, row = edge_index[0], edge_index[1]\n",
    "\n",
    "        deg = scatter(value, col, 0, dim_size=num_nodes, reduce='sum')\n",
    "        deg_inv_sqrt = deg.pow_(-0.5)\n",
    "        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n",
    "        value = deg_inv_sqrt[row] * value * deg_inv_sqrt[col]\n",
    "\n",
    "        return set_sparse_value(adj_t, value), None\n",
    "\n",
    "    assert flow in ['source_to_target', 'target_to_source']\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    if add_self_loops:\n",
    "        edge_index, edge_weight = add_remaining_self_loops(\n",
    "            edge_index, edge_weight, fill_value, num_nodes)\n",
    "\n",
    "    if edge_weight is None:\n",
    "        edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype,\n",
    "                                 device=edge_index.device)\n",
    "\n",
    "    row, col = edge_index[0], edge_index[1]\n",
    "    idx = col if flow == 'source_to_target' else row\n",
    "    deg = scatter(edge_weight, idx, dim=0, dim_size=num_nodes, reduce='sum')\n",
    "    deg_inv_sqrt = deg.pow_(-0.5)\n",
    "    deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n",
    "    edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "class test_GCNConv(MessagePassing):\n",
    "    r\"\"\"The graph convolutional operator from the `\"Semi-supervised\n",
    "    Classification with Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1609.02907>`_ paper.\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{X}^{\\prime} = \\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "        \\mathbf{\\hat{D}}^{-1/2} \\mathbf{X} \\mathbf{\\Theta},\n",
    "\n",
    "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\n",
    "    adjacency matrix with inserted self-loops and\n",
    "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\n",
    "    The adjacency matrix can include other values than :obj:`1` representing\n",
    "    edge weights via the optional :obj:`edge_weight` tensor.\n",
    "\n",
    "    Its node-wise formulation is given by:\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\mathbf{\\Theta}^{\\top} \\sum_{j \\in\n",
    "        \\mathcal{N}(i) \\cup \\{ i \\}} \\frac{e_{j,i}}{\\sqrt{\\hat{d}_j\n",
    "        \\hat{d}_i}} \\mathbf{x}_j\n",
    "\n",
    "    with :math:`\\hat{d}_i = 1 + \\sum_{j \\in \\mathcal{N}(i)} e_{j,i}`, where\n",
    "    :math:`e_{j,i}` denotes the edge weight from source node :obj:`j` to target\n",
    "    node :obj:`i` (default: :obj:`1.0`)\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample, or :obj:`-1` to derive\n",
    "            the size from the first input(s) to the forward method.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        improved (bool, optional): If set to :obj:`True`, the layer computes\n",
    "            :math:`\\mathbf{\\hat{A}}` as :math:`\\mathbf{A} + 2\\mathbf{I}`.\n",
    "            (default: :obj:`False`)\n",
    "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
    "            the computation of :math:`\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "            \\mathbf{\\hat{D}}^{-1/2}` on first execution, and will use the\n",
    "            cached version for further executions.\n",
    "            This parameter should only be set to :obj:`True` in transductive\n",
    "            learning scenarios. (default: :obj:`False`)\n",
    "        add_self_loops (bool, optional): If set to :obj:`False`, will not add\n",
    "            self-loops to the input graph. By default, self-loops will be added\n",
    "            in case :obj:`normalize` is set to :obj:`True`, and not added\n",
    "            otherwise. (default: :obj:`None`)\n",
    "        normalize (bool, optional): Whether to add self-loops and compute\n",
    "            symmetric normalization coefficients on-the-fly.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "\n",
    "    Shapes:\n",
    "        - **input:**\n",
    "          node features :math:`(|\\mathcal{V}|, F_{in})`,\n",
    "          edge indices :math:`(2, |\\mathcal{E}|)`\n",
    "          or sparse matrix :math:`(|\\mathcal{V}|, |\\mathcal{V}|)`,\n",
    "          edge weights :math:`(|\\mathcal{E}|)` *(optional)*\n",
    "        - **output:** node features :math:`(|\\mathcal{V}|, F_{out})`\n",
    "    \"\"\"\n",
    "    _cached_edge_index: Optional[OptPairTensor]\n",
    "    _cached_adj_t: Optional[SparseTensor]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        improved: bool = False,\n",
    "        cached: bool = False,\n",
    "        add_self_loops: Optional[bool] = None,\n",
    "        normalize: bool = True,\n",
    "        bias: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if add_self_loops is None:\n",
    "            add_self_loops = normalize\n",
    "\n",
    "        if add_self_loops and not normalize:\n",
    "            raise ValueError(f\"'{self.__class__.__name__}' does not support \"\n",
    "                             f\"adding self-loops to the graph when no \"\n",
    "                             f\"on-the-fly normalization is applied\")\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self._cached_edge_index = None\n",
    "        self._cached_adj_t = None\n",
    "\n",
    "        self.lin = Linear(in_channels, out_channels, bias=False,\n",
    "                          weight_initializer='glorot')\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        self.lin.reset_parameters()\n",
    "        zeros(self.bias)\n",
    "        self._cached_edge_index = None\n",
    "        self._cached_adj_t = None\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj,\n",
    "                edge_weight: OptTensor = None) -> Tensor:\n",
    "\n",
    "        if isinstance(x, (tuple, list)):\n",
    "            raise ValueError(f\"'{self.__class__.__name__}' received a tuple \"\n",
    "                             f\"of node features as input while this layer \"\n",
    "                             f\"does not support bipartite message passing. \"\n",
    "                             f\"Please try other layers such as 'SAGEConv' or \"\n",
    "                             f\"'GraphConv' instead\")\n",
    "\n",
    "        if self.normalize:\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                cache = self._cached_edge_index\n",
    "                if cache is None:\n",
    "                    edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
    "                        edge_index, edge_weight, x.size(self.node_dim),\n",
    "                        self.improved, self.add_self_loops, self.flow, x.dtype)\n",
    "                    if self.cached:\n",
    "                        self._cached_edge_index = (edge_index, edge_weight)\n",
    "                else:\n",
    "                    edge_index, edge_weight = cache[0], cache[1]\n",
    "\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                cache = self._cached_adj_t\n",
    "                if cache is None:\n",
    "                    edge_index = gcn_norm(  # yapf: disable\n",
    "                        edge_index, edge_weight, x.size(self.node_dim),\n",
    "                        self.improved, self.add_self_loops, self.flow, x.dtype)\n",
    "                    if self.cached:\n",
    "                        self._cached_adj_t = edge_index\n",
    "                else:\n",
    "                    edge_index = cache\n",
    "\n",
    "        # x = self.lin(x)\n",
    "\n",
    "        # propagate_type: (x: Tensor, edge_weight: OptTensor)\n",
    "        out = self.propagate(edge_index, x=x, edge_weight=edge_weight)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_weight: OptTensor) -> Tensor:\n",
    "        return x_j if edge_weight is None else edge_weight.view(-1, 1) * x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: Adj, x: Tensor) -> Tensor:\n",
    "        return spmm(adj_t, x, reduce=self.aggr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bc973562-6376-4809-9780-c3fc5bdc53b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleHGCN_GC(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleHGCN_GC, self).__init__()\n",
    "        self.n2e = test_HGConv_node_to_edge(in_channels=1, out_channels=1, use_attention=False)\n",
    "        self.e2n = test_HGConv_edge_to_node(in_channels=1, out_channels=1, use_attention=False)\n",
    "        self.gcn = test_GCNConv(in_channels=1, out_channels=1)\n",
    "    \n",
    "    def forward(self, x, edge_index, hyperedge_index):\n",
    "        # 节点到超边的信息传递\n",
    "        hyperedge_feats = self.n2e(x, hyperedge_index)\n",
    "\n",
    "        new_hyperedge_feats = self.gcn(hyperedge_feats[0], edge_index)\n",
    "        # 超边到节点的信息传递\n",
    "        node_feats = self.e2n(new_hyperedge_feats, hyperedge_index)\n",
    "        # return hyperedge_feats\n",
    "        # return hyperedge_feats, new_hyperedge_feats\n",
    "        return hyperedge_feats, new_hyperedge_feats, node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c033ff4d-8881-43c2-b0b8-218ef27cb9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :  tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.]])\n",
      "out :  tensor([[[2.]],\n",
      "\n",
      "        [[3.]],\n",
      "\n",
      "        [[5.]]])\n",
      "Node to Hyperedge Information Transfer Result:\n",
      "(tensor([[2.],\n",
      "        [3.],\n",
      "        [5.]], grad_fn=<AddBackward0>), 3, 6, tensor([1., 1., 1.]))\n",
      "\n",
      " new Hyperedge to Node Information Transfer Result:\n",
      "tensor([[2.2247],\n",
      "        [3.8577],\n",
      "        [3.7247]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Hyperedge to Node Information Transfer Result:\n",
      "tensor([[2.2247],\n",
      "        [3.8577],\n",
      "        [3.0412],\n",
      "        [3.7912],\n",
      "        [3.7247],\n",
      "        [3.7247]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HypergraphConv\n",
    "\n",
    "# 构建 6 个节点和 3 个超边的 incident matrix\n",
    "num_nodes = 6\n",
    "num_hyperedges = 3\n",
    "\n",
    "# incident matrix shape: (num_nodes, num_hyperedges)\n",
    "# 假设以下 incident matrix (手动设置一个示例)\n",
    "incidence_matrix = torch.tensor([\n",
    "    [1, 0, 0],  # node 1 connected to hyperedge 1\n",
    "    [0, 1, 0],  # node 2 connected to hyperedge 1 and 2\n",
    "    [1, 1, 0],  # node 3 connected to hyperedge 2\n",
    "    [0, 1, 1],  # node 4 connected to hyperedge 2 and 3\n",
    "    [0, 0, 1],  # node 5 connected to hyperedge 3\n",
    "    [0, 0, 1]   # node 6 connected to hyperedge 1 and 3\n",
    "], dtype=torch.float)\n",
    "\n",
    "# 转换为边索引\n",
    "hyperedge_index = incidence_matrix.nonzero().t()\n",
    "\n",
    "adjacency_matrix = torch.tensor([\n",
    "    [1, 1, 0],  # node 1 has a self-loop\n",
    "    [1, 1, 1],  # node 2 has a self-loop and connects to node 3\n",
    "    [0, 1, 1]   # node 3 has a self-loop and connects to node 2\n",
    "], dtype=torch.float)\n",
    "\n",
    "# 将邻接矩阵转换为边索引\n",
    "edge_index = adjacency_matrix.nonzero(as_tuple=False).t()\n",
    "\n",
    "\n",
    "# 节点特征 (假设 6 个节点，每个节点特征维度为 1，且初始值为 1)\n",
    "# x = torch.ones((num_nodes, 1))\n",
    "x = torch.arange(1, num_nodes + 1, dtype=torch.float).view(-1, 1)\n",
    "print(\"x : \", x)\n",
    "# 初始化模型\n",
    "model = SimpleHGCN_GC()\n",
    "\n",
    "# 前向传播\n",
    "# node_feats = model(x, edge_index, hyperedge_index)\n",
    "# hyperedge_feats, new_hyperedge_feats = model(x, edge_index, hyperedge_index)\n",
    "hyperedge_feats, new_hyperedge_feats, node_feats = model(x, edge_index, hyperedge_index)\n",
    "# hyperedge_feats = model(x, edge_index)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Node to Hyperedge Information Transfer Result:\")\n",
    "print(hyperedge_feats)\n",
    "\n",
    "print(\"\\n new Hyperedge to Node Information Transfer Result:\")\n",
    "print(new_hyperedge_feats)\n",
    "\n",
    "\n",
    "print(\"\\nHyperedge to Node Information Transfer Result:\")\n",
    "print(node_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "d17196ab-3281-4aa4-a52c-3409d3b6038c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XH:\n",
      "[v₁ + v₃  v₂ + v₃ + v₄  v₄ + v₅ + v₆]\n",
      "\n",
      "Diagonal Matrix B:\n",
      "⎡2  0  0⎤\n",
      "⎢       ⎥\n",
      "⎢0  3  0⎥\n",
      "⎢       ⎥\n",
      "⎣0  0  3⎦\n",
      "\n",
      "Diagonal Matrix D:\n",
      "⎡1  0  0  0  0  0⎤\n",
      "⎢                ⎥\n",
      "⎢0  1  0  0  0  0⎥\n",
      "⎢                ⎥\n",
      "⎢0  0  2  0  0  0⎥\n",
      "⎢                ⎥\n",
      "⎢0  0  0  2  0  0⎥\n",
      "⎢                ⎥\n",
      "⎢0  0  0  0  1  0⎥\n",
      "⎢                ⎥\n",
      "⎣0  0  0  0  0  1⎦\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# 定义符号\n",
    "a, b, c, d, e, f = sp.symbols('v1 v2 v3 v4 v5 v6')\n",
    "\n",
    "# 定义矩阵 A\n",
    "H = sp.Matrix([\n",
    "    [1, 0, 0],  # node 1 connected to hyperedge 1\n",
    "    [0, 1, 0],  # node 2 connected to hyperedge 1 and 2\n",
    "    [1, 1, 0],  # node 3 connected to hyperedge 2\n",
    "    [0, 1, 1],  # node 4 connected to hyperedge 2 and 3\n",
    "    [0, 0, 1],  # node 5 connected to hyperedge 3\n",
    "    [0, 0, 1]   # node 6 connected to hyperedge 1 and 3\n",
    "])\n",
    "\n",
    "A = sp.Matrix([\n",
    "    [1, 1, 0],  # node 1 connected to hyperedge 1\n",
    "    [1, 1, 1],  # node 2 connected to hyperedge 1 and 2\n",
    "    [0, 1, 1]  # node 3 connected to hyperedge 2\n",
    "])\n",
    "\n",
    "# 定义矩阵 X\n",
    "X = sp.Matrix([\n",
    "    [a, b, c, d, e,f],\n",
    "])\n",
    "# X = sp.Matrix([\n",
    "#     [1, 2, 3, 4, 5, 6],\n",
    "# ])\n",
    "\n",
    "\n",
    "# 一個節點有幾個 hyperedge\n",
    "hyperedges_of_node = [sum(H.row(i)) for i in range(H.rows)]\n",
    "# 一個 hyperedge 有幾個 node\n",
    "nodes_of_hyperedges = [sum(H.col(i)) for i in range(H.cols)]\n",
    "node_degree = [sum(A.row(i)) for i in range(A.rows)]\n",
    "\n",
    "D = sp.diag(*hyperedges_of_node)\n",
    "AD = sp.diag(*node_degree)\n",
    "B = sp.diag(*nodes_of_hyperedges)\n",
    "\n",
    "\n",
    "# 计算 AXW\n",
    "XH = X * H\n",
    "\n",
    "\n",
    "print(\"XH:\")\n",
    "sp.pprint(XH)\n",
    "print(\"\\nDiagonal Matrix B:\")\n",
    "sp.pprint(B)\n",
    "\n",
    "print(\"\\nDiagonal Matrix D:\")\n",
    "sp.pprint(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c4b7ee17-61cd-408a-bf86-bf4a63b19540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagonal Matrix B:\n",
      "⎡1  0  0  0  0  0⎤\n",
      "⎢                ⎥\n",
      "⎢0  1  0  0  0  0⎥\n",
      "⎢                ⎥\n",
      "⎢0  0  2  0  0  0⎥\n",
      "⎢                ⎥\n",
      "⎢0  0  0  2  0  0⎥\n",
      "⎢                ⎥\n",
      "⎢0  0  0  0  1  0⎥\n",
      "⎢                ⎥\n",
      "⎣0  0  0  0  0  1⎦\n",
      "\n",
      "B^(-1/2):\n",
      "⎡√2        ⎤\n",
      "⎢──  0   0 ⎥\n",
      "⎢2         ⎥\n",
      "⎢          ⎥\n",
      "⎢    √3    ⎥\n",
      "⎢0   ──  0 ⎥\n",
      "⎢    3     ⎥\n",
      "⎢          ⎥\n",
      "⎢        √2⎥\n",
      "⎢0   0   ──⎥\n",
      "⎣        2 ⎦\n"
     ]
    }
   ],
   "source": [
    "# AD_neg_half = AD.applyfunc(lambda x: 0 if x == 0 else x**(-1/2))\n",
    "# AD_neg_half = AD.applyfunc(lambda x: 0 if x == 0 else sp.Rational(1, x)**(1/2))\n",
    "# AD_neg_half = AD.applyfunc(lambda x: sp.Rational(0, 1) if x == 0 else sp.Rational(1, x)**(1/2))\n",
    "AD_neg_half = AD.applyfunc(lambda x: sp.Rational(0, 1) if x == 0 else sp.sqrt(sp.Rational(1, x)))\n",
    "D_neg = D.applyfunc(lambda x: 0 if x == 0 else x**(-1))\n",
    "B_neg = B.applyfunc(lambda x: 0 if x == 0 else x**(-1))\n",
    "print(\"Diagonal Matrix B:\")\n",
    "sp.pprint(D)\n",
    "\n",
    "print(\"\\nB^(-1/2):\")\n",
    "sp.pprint(AD_neg_half)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8738c208-cef5-4d22-bfb0-3e95abaf4cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGCN hyperedge feature E : \n",
      "⎡  v₁   v₃   ⎤\n",
      "⎢  ── + ──   ⎥\n",
      "⎢  2    2    ⎥\n",
      "⎢            ⎥\n",
      "⎢v₂   v₃   v₄⎥\n",
      "⎢── + ── + ──⎥\n",
      "⎢3    3    3 ⎥\n",
      "⎢            ⎥\n",
      "⎢v₄   v₅   v₆⎥\n",
      "⎢── + ── + ──⎥\n",
      "⎣3    3    3 ⎦\n",
      "HGCN next node feature X : \n",
      "⎡       v₁   v₃        ⎤\n",
      "⎢       ── + ──        ⎥\n",
      "⎢       2    2         ⎥\n",
      "⎢                      ⎥\n",
      "⎢     v₂   v₃   v₄     ⎥\n",
      "⎢     ── + ── + ──     ⎥\n",
      "⎢     3    3    3      ⎥\n",
      "⎢                      ⎥\n",
      "⎢ v₁   v₂   5⋅v₃   v₄  ⎥\n",
      "⎢ ── + ── + ──── + ──  ⎥\n",
      "⎢ 4    6     12    6   ⎥\n",
      "⎢                      ⎥\n",
      "⎢v₂   v₃   v₄   v₅   v₆⎥\n",
      "⎢── + ── + ── + ── + ──⎥\n",
      "⎢6    6    3    6    6 ⎥\n",
      "⎢                      ⎥\n",
      "⎢     v₄   v₅   v₆     ⎥\n",
      "⎢     ── + ── + ──     ⎥\n",
      "⎢     3    3    3      ⎥\n",
      "⎢                      ⎥\n",
      "⎢     v₄   v₅   v₆     ⎥\n",
      "⎢     ── + ── + ──     ⎥\n",
      "⎣     3    3    3      ⎦\n"
     ]
    }
   ],
   "source": [
    "print(\"HGCN hyperedge feature E : \")\n",
    "E = B_neg * H.transpose() * X.transpose()\n",
    "sp.pprint(E)\n",
    "print(\"HGCN next node feature X : \")\n",
    "next_x = D_neg * H * B_neg * H.transpose() * X.transpose()\n",
    "sp.pprint(next_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a2912b8b-c53a-45a8-b4f9-859273708970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⎡        3⋅v₁   v₂   11⋅v₃   v₄         ⎤\n",
      "⎢        ──── + ── + ───── + ──         ⎥\n",
      "⎢         8     12     24    12         ⎥\n",
      "⎢                                       ⎥\n",
      "⎢  v₁   2⋅v₂   11⋅v₃   5⋅v₄   v₅   v₆   ⎥\n",
      "⎢  ── + ──── + ───── + ──── + ── + ──   ⎥\n",
      "⎢  12    9       36     18    18   18   ⎥\n",
      "⎢                                       ⎥\n",
      "⎢11⋅v₁   11⋅v₂   55⋅v₃   13⋅v₄   v₅   v₆⎥\n",
      "⎢───── + ───── + ───── + ───── + ── + ──⎥\n",
      "⎢  48      72     144      72    36   36⎥\n",
      "⎢                                       ⎥\n",
      "⎢  v₁   5⋅v₂   13⋅v₃   11⋅v₄   v₅   v₆  ⎥\n",
      "⎢  ── + ──── + ───── + ───── + ── + ──  ⎥\n",
      "⎢  24    36      72      36    6    6   ⎥\n",
      "⎢                                       ⎥\n",
      "⎢      v₂   v₃   v₄   5⋅v₅   5⋅v₆       ⎥\n",
      "⎢      ── + ── + ── + ──── + ────       ⎥\n",
      "⎢      18   18   3     18     18        ⎥\n",
      "⎢                                       ⎥\n",
      "⎢      v₂   v₃   v₄   5⋅v₅   5⋅v₆       ⎥\n",
      "⎢      ── + ── + ── + ──── + ────       ⎥\n",
      "⎣      18   18   3     18     18        ⎦\n"
     ]
    }
   ],
   "source": [
    "nn_x = D_neg * H * B_neg * H.transpose() * next_x\n",
    "sp.pprint(nn_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b9f23854-db80-4b97-be8a-435943897a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGCN hyperedge feature E : \n",
      "⎡  v₁   v₃   ⎤\n",
      "⎢  ── + ──   ⎥\n",
      "⎢  2    2    ⎥\n",
      "⎢            ⎥\n",
      "⎢v₂   v₃   v₄⎥\n",
      "⎢── + ── + ──⎥\n",
      "⎢3    3    3 ⎥\n",
      "⎢            ⎥\n",
      "⎢v₄   v₅   v₆⎥\n",
      "⎢── + ── + ──⎥\n",
      "⎣3    3    3 ⎦\n",
      "HGCN_GC next node feature X_GC : \n",
      "⎡                       ⎛v₂   v₃   v₄⎞          ⎤\n",
      "⎢                    √6⋅⎜── + ── + ──⎟          ⎥\n",
      "⎢          v₁   v₃      ⎝3    3    3 ⎠          ⎥\n",
      "⎢          ── + ── + ─────────────────          ⎥\n",
      "⎢          4    4            6                  ⎥\n",
      "⎢                                               ⎥\n",
      "⎢                  ⎛v₁   v₃⎞      ⎛v₄   v₅   v₆⎞⎥\n",
      "⎢               √6⋅⎜── + ──⎟   √6⋅⎜── + ── + ──⎟⎥\n",
      "⎢v₂   v₃   v₄      ⎝2    2 ⎠      ⎝3    3    3 ⎠⎥\n",
      "⎢── + ── + ── + ──────────── + ─────────────────⎥\n",
      "⎢9    9    9         6                 6        ⎥\n",
      "⎢                                               ⎥\n",
      "⎢                         ⎛v₂   v₃   v₄⎞        ⎥\n",
      "⎢                      √6⋅⎜── + ── + ──⎟        ⎥\n",
      "⎢       v₄   v₅   v₆      ⎝3    3    3 ⎠        ⎥\n",
      "⎢       ── + ── + ── + ─────────────────        ⎥\n",
      "⎣       6    6    6            6                ⎦\n",
      "HGCN next node feature X : \n",
      "⎡                                       ⎛v₂   v₃   v₄⎞                        \n",
      "⎢                                    √6⋅⎜── + ── + ──⎟                        \n",
      "⎢                          v₁   v₃      ⎝3    3    3 ⎠                        \n",
      "⎢                          ── + ── + ─────────────────                        \n",
      "⎢                          4    4            6                                \n",
      "⎢                                                                             \n",
      "⎢                                  ⎛v₁   v₃⎞      ⎛v₄   v₅   v₆⎞              \n",
      "⎢                               √6⋅⎜── + ──⎟   √6⋅⎜── + ── + ──⎟              \n",
      "⎢                v₂   v₃   v₄      ⎝2    2 ⎠      ⎝3    3    3 ⎠              \n",
      "⎢                ── + ── + ── + ──────────── + ─────────────────              \n",
      "⎢                9    9    9         6                 6                      \n",
      "⎢                                                                             \n",
      "⎢                            ⎛v₁   v₃⎞      ⎛v₂   v₃   v₄⎞      ⎛v₄   v₅   v₆⎞\n",
      "⎢                         √6⋅⎜── + ──⎟   √6⋅⎜── + ── + ──⎟   √6⋅⎜── + ── + ──⎟\n",
      "⎢  v₁   v₂   13⋅v₃   v₄      ⎝2    2 ⎠      ⎝3    3    3 ⎠      ⎝3    3    3 ⎠\n",
      "⎢  ── + ── + ───── + ── + ──────────── + ───────────────── + ─────────────────\n",
      "⎢  8    18     72    18        12                12                  12       \n",
      "⎢                                                                             \n",
      "⎢                              ⎛v₁   v₃⎞      ⎛v₂   v₃   v₄⎞      ⎛v₄   v₅   v\n",
      "⎢                           √6⋅⎜── + ──⎟   √6⋅⎜── + ── + ──⎟   √6⋅⎜── + ── + ─\n",
      "⎢v₂   v₃   5⋅v₄   v₅   v₆      ⎝2    2 ⎠      ⎝3    3    3 ⎠      ⎝3    3    3\n",
      "⎢── + ── + ──── + ── + ── + ──────────── + ───────────────── + ───────────────\n",
      "⎢18   18    36    12   12        12                12                  12     \n",
      "⎢                                                                             \n",
      "⎢                                         ⎛v₂   v₃   v₄⎞                      \n",
      "⎢                                      √6⋅⎜── + ── + ──⎟                      \n",
      "⎢                       v₄   v₅   v₆      ⎝3    3    3 ⎠                      \n",
      "⎢                       ── + ── + ── + ─────────────────                      \n",
      "⎢                       6    6    6            6                              \n",
      "⎢                                                                             \n",
      "⎢                                         ⎛v₂   v₃   v₄⎞                      \n",
      "⎢                                      √6⋅⎜── + ── + ──⎟                      \n",
      "⎢                       v₄   v₅   v₆      ⎝3    3    3 ⎠                      \n",
      "⎢                       ── + ── + ── + ─────────────────                      \n",
      "⎣                       6    6    6            6                              \n",
      "\n",
      "  ⎤\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "₆⎞⎥\n",
      "─⎟⎥\n",
      " ⎠⎥\n",
      "──⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎥\n",
      "  ⎦\n"
     ]
    }
   ],
   "source": [
    "print(\"HGCN_GC hyperedge feature E : \")\n",
    "E = B_neg * H.transpose() * X.transpose()\n",
    "sp.pprint(E)\n",
    "print(\"HGCN_GC next node feature X_GC : \")\n",
    "NE =  AD_neg_half * A * AD_neg_half * E\n",
    "sp.pprint(NE)\n",
    "print(\"HGCN_GC next node feature X : \")\n",
    "next_x = D_neg * H * NE\n",
    "sp.pprint(next_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e71f0d-bc17-4241-b077-3f8b1ef4ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgcn_gc_x = D_neg * H * B_neg * H.transpose() * X.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb0fd75-38ca-48c0-ac4b-59d66098392c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
